{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95808f4f",
   "metadata": {},
   "source": [
    "# MMPose — Video Pose Demo (YOLO/OpenCV-style overlay)\n",
    "\n",
    "This notebook installs **MMPose** and runs 2D pose estimation on an input video,\n",
    "then draws a simple white **nodes + connections + labels** overlay on a dimmed, blurred background.\n",
    "\n",
    "Works in **Google Colab** or local Jupyter. Just set `input_path` and `output_path` and run the last cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b450b",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842c556",
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Step 1: Install dependencies (Colab/Jupyter)\n",
    "%%bash\n",
    "pip -q install -U openmim\n",
    "mim -q install 'mmengine>=0.10.0' 'mmcv>=2.0.0' 'mmdet>=3.3.0'\n",
    "pip -q install -U mmpose opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe708ee2",
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Step 2: Imports\n",
    "import os, math\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import display, Video\n",
    "from mmpose.apis import MMPoseInferencer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a05486c",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c12bb",
   "metadata": {
    "id": "paths"
   },
   "outputs": [],
   "source": [
    "# Edit these paths\n",
    "input_path  = '/content/input.mp4'  # <-- set your input video path\n",
    "output_path = '/content/mmpose_out.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40d4a5",
   "metadata": {},
   "source": [
    "### Visualization settings (white nodes/links + labels on dimmed/blurred background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b5b5d",
   "metadata": {
    "id": "viz_config"
   },
   "outputs": [],
   "source": [
    "# Config for the simple visualization\n",
    "conf_thresh = 0.1       # minimum keypoint confidence to draw\n",
    "node_radius = 2         # circle radius for each node\n",
    "line_thickness = 1      # line thickness for connections\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 0.35\n",
    "font_thickness = 1\n",
    "\n",
    "# Background styling\n",
    "bg_blur_sigma   = 7     # Gaussian blur sigma for background\n",
    "bg_dim_factor   = 0.35  # 0..1, multiply background brightness\n",
    "\n",
    "# Optional: double-stroke lines (wide faint band under thin line)\n",
    "wide_line_px = 10\n",
    "wide_line_alpha = 0.3\n",
    "\n",
    "# COCO keypoint names (17 points)\n",
    "KP_NAMES = [\n",
    "    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
    "    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
    "]\n",
    "\n",
    "# Skeleton edges (pairs of indices)\n",
    "SKELETON = [\n",
    "    (5, 6),                 # shoulders\n",
    "    (5, 7), (7, 9),         # left arm\n",
    "    (6, 8), (8,10),         # right arm\n",
    "    (11,12),                # hips\n",
    "    (5,11), (6,12),         # torso\n",
    "    (11,13), (13,15),       # left leg\n",
    "    (12,14), (14,16),       # right leg\n",
    "    (0,1), (0,2), (1,3), (2,4)  # head/face links\n",
    "]\n",
    "\n",
    "def dim_and_blur(bgr, sigma=7, dim=0.35):\n",
    "    out = bgr\n",
    "    if sigma > 0:\n",
    "        out = cv2.GaussianBlur(out, (0, 0), sigma)\n",
    "    out = np.clip(out.astype(np.float32) * float(dim), 0, 255).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "def draw_pose_overlay(base_bgr, kpts_xyc, overlay=None):\n",
    "    \"\"\"Draw white nodes + connections + labels on overlay (or copy of base frame if overlay is None).\n",
    "    kpts_xyc: np.ndarray [num_points, 3] -> (x,y,conf)\n",
    "    \"\"\"\n",
    "    overlay = base_bgr.copy() if overlay is None else overlay\n",
    "\n",
    "    # Lines with double-stroke (wide faint band + thin crisp line)\n",
    "    for a, b in SKELETON:\n",
    "        if a >= kpts_xyc.shape[0] or b >= kpts_xyc.shape[0]:\n",
    "            continue\n",
    "        x1, y1, c1 = kpts_xyc[a]\n",
    "        x2, y2, c2 = kpts_xyc[b]\n",
    "        if c1 < conf_thresh or c2 < conf_thresh:\n",
    "            continue\n",
    "        # wide, faint band\n",
    "        tmp = overlay.copy()\n",
    "        cv2.line(tmp, (int(x1), int(y1)), (int(x2), int(y2)), (255,255,255), wide_line_px, lineType=cv2.LINE_AA)\n",
    "        cv2.addWeighted(tmp, wide_line_alpha, overlay, 1.0-wide_line_alpha, 0, overlay)\n",
    "        # thin crisp line\n",
    "        cv2.line(overlay, (int(x1), int(y1)), (int(x2), int(y2)), (255,255,255), line_thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # Nodes + labels\n",
    "    for i, (x, y, c) in enumerate(kpts_xyc):\n",
    "        if c < conf_thresh:\n",
    "            continue\n",
    "        cv2.circle(overlay, (int(x), int(y)), node_radius, (255,255,255), -1, lineType=cv2.LINE_AA)\n",
    "        name = KP_NAMES[i] if i < len(KP_NAMES) else f'k{i}'\n",
    "        tx, ty = int(x) + 6, max(12, int(y) - 6)\n",
    "        cv2.putText(overlay, name, (tx, ty), font, font_scale, (255,255,255), font_thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d43b6",
   "metadata": {},
   "source": [
    "### Inference + Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb048a41",
   "metadata": {
    "id": "infer_render"
   },
   "outputs": [],
   "source": [
    "def infer_and_render_mmpose_video(input_video_path, output_path,\n",
    "                                  pose_alias='rtmpose-s', det_alias='rtmdet-tiny'):\n",
    "    \"\"\"\n",
    "    Runs MMPose (with internal detector) on a video and writes a rendered MP4.\n",
    "    - pose_alias: MMPose model alias (e.g., 'rtmpose-s', 'rtmpose-m').\n",
    "    - det_alias: detector alias (e.g., 'rtmdet-tiny', 'rtmdet-s').\n",
    "    \"\"\"\n",
    "    # Initialize MMPose high-level inferencer. It will auto-download weights.\n",
    "    inferencer = MMPoseInferencer(pose2d=pose_alias, det_model=det_alias)\n",
    "\n",
    "    # Probe video size & fps\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f'Cannot open video: {input_video_path}')\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "    cap.release()\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    assert writer.isOpened(), f'Could not open VideoWriter for {output_path}'\n",
    "\n",
    "    # The inferencer takes a video path and yields result dicts per frame\n",
    "    results_gen = inferencer(input_video_path, stream=True)\n",
    "\n",
    "    for idx, res in enumerate(results_gen):\n",
    "        # res contains: 'predictions' (list per instance) and may include 'frame'\n",
    "        frame_bgr = res.get('frame', None)\n",
    "        if frame_bgr is None:\n",
    "            # Safety fallback — skip if no frame present (shouldn't happen with inferencer)\n",
    "            continue\n",
    "\n",
    "        # Prepare the background (dim + blur)\n",
    "        bg = dim_and_blur(frame_bgr, sigma=bg_blur_sigma, dim=bg_dim_factor)\n",
    "        overlay = bg.copy()\n",
    "\n",
    "        preds = res.get('predictions', [])\n",
    "        if preds is None:\n",
    "            preds = []\n",
    "\n",
    "        # Draw all detected people in the frame\n",
    "        for det in preds:\n",
    "            # 'det' usually has: 'keypoints' (Kx2), 'keypoint_scores' (K,), etc.\n",
    "            kxy = det.get('keypoints', None)\n",
    "            ksc = det.get('keypoint_scores', None)\n",
    "            if kxy is None:\n",
    "                continue\n",
    "            kxy = np.array(kxy, dtype=np.float32)\n",
    "            if ksc is None:\n",
    "                ksc = np.ones((kxy.shape[0],), dtype=np.float32)\n",
    "            else:\n",
    "                ksc = np.array(ksc, dtype=np.float32)\n",
    "            if ksc.ndim == 1:\n",
    "                ksc = ksc[:, None]\n",
    "            kpts_xyc = np.concatenate([kxy, ksc], axis=1).astype(np.float32)\n",
    "            overlay = draw_pose_overlay(bg, kpts_xyc, overlay=overlay)\n",
    "\n",
    "        writer.write(overlay)\n",
    "\n",
    "    writer.release()\n",
    "    print('[OK] Wrote:', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd79bd3",
   "metadata": {
    "id": "run_and_preview"
   },
   "outputs": [],
   "source": [
    "# Run it\n",
    "infer_and_render_mmpose_video(input_path, output_path,\n",
    "                              pose_alias='rtmpose-s', det_alias='rtmdet-tiny')\n",
    "\n",
    "# Preview inline (autoplay loop)\n",
    "if os.path.exists(output_path):\n",
    "    display(Video(output_path, embed=True, html_attributes='autoplay loop controls'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
